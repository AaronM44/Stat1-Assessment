---
title: "Stats1 Assessment"
author: "Aaron Munro"
date: "2022-12-10"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Main Findings

- A weak positive correlation between median temperature and beer consumption exists.
- For an increase in temperature of 1 degree Celsius we would expect an increase in beer consumption of 0.2 litres.
- Linear regression analysis produced a low r-squared value suggesting the relationship does not sufficiently explain the variance in the data.
- Simple linear regression model resulted in a p-value of 0.0006445 suggesting that the relationship is significant.
- Multiple regression model showed improvement in p-value and adjusted r-squared but not significantly.
- Multiple regression model introduced multicollinear variables which could produce misleading results.
- Sample of data possibly too small to be representative of population.

## Executive Summary

In 2015 data was gathered related to the volume of beer consumed by groups of students in Sao Paulo, Brazil and the weather across the year. This report aimed to understand whether there was a relationship between temperature and the amount of beer consumed. Linear regression was chosen as the method to describe the relationship as the variables in question were both quantitative. The result was a very weak positive linear relationship that showed as the temperature increases by 1 degree Celsius beer consumption also increases by around 0.2 litres. However, the data also contained a large amount of variance which was shown to be the case via a low R-Squared value. Moving to a multiple linear regression model with median, minimum and maximum temperature produced a very slight improvement when compared to the simple regression model but it was not significant enough to change the assessment. The extra complexity and dubious use of variables displaying multicollinearity would also lead me to rule out this model and stick with the original simple regression version.

## Introduction

The dataset chosen for the assessment was found on Kaggle (Lustosa 2018). Over a period of a year in 2015, data was collected containing the beer consumption measured in litres of an unknown number of students around a university area in Sao Paulo, Brazil. The dataset contained three variables related to temperature: median, minimum and maximum recorded for each day, these were all measured in Celsius. Precipitation was also recorded for each day and measured by the total volume in millimeters. Finally, the date was recorded along with a categorical field for whether the date fell on a weekday or weekend. There is one piece of data missing that I would consider crucial to an analysis of this kind and that is the number of students data was collected from. Without this it is difficult to determine whether the data is representative of the population. It is also unclear whether data was collected from the same number of students each day or whether this fluctuated over the year. 

The original columns were in Portugese and were translated for this project using Google. This does not affect results but will make the analysis easier to understand for the intended audience:

- Data - Date
- Temperatura Media (C) - Median Temp. (C)
- Temperatura Minima (C) - Min. Temp (C)
- Temperatura Maxima (C) - Max. Temp (C)
- Precipitacao (mm) - Precipitation (mm)
- Final de Semana - Weekend
- Consumo de cerveja (litros) - Consumption (litres)


### Hypothesis

The question we will seek to understand in this report is whether there is a relationship between temperature and beer consumption among students in Sao Paulo, Brazil?

H0 - There is no significant relationship between temperature and beer consumption among students in Sao Paulo.

H1 - There is a significant relationship between temperature and beer consumption among students in Sao Paulo.

``` {r imports, echo=FALSE}
# Import packages
library(tidyverse)
library(broom)
library(patchwork)
library(lubridate)
library(corrplot)
library(tidymodels)
library(ggfortify)
```

### Original Dataset Analysis

The first task was to plot the distribution of each variable in the source dataset to gain an understanding of the shape of data. We then looked at each of these at a monthly level. This would help form the strategy for simulating a new dataset.

``` {r orig_read, echo=FALSE}
# Import data - dataset has commas instead of decimal points and many blank rows
orig_data <- read.csv("data/Consumo_cerveja.csv", dec = ",")
orig_data <- orig_data[1:365,]

set.seed(7)
```

``` {r orig_rename, echo=FALSE}
# Rename columns
orig_data <- orig_data %>%
  rename(date = Data,
         temp_median = Temperatura.Media..C.,
         temp_min = Temperatura.Minima..C.,
         temp_max = Temperatura.Maxima..C.,
         precip = Precipitacao..mm.,
         weekend = Final.de.Semana,
         consum = Consumo.de.cerveja..litros.)
```

``` {r orig_head, echo=FALSE}
head(orig_data)
```

``` {r orig_comma_rep, echo=FALSE}
# Format fields correctly
orig_data$date <- as_date(as.Date(orig_data$date))
orig_data$temp_median <- as.numeric(orig_data$temp_median)
orig_data$temp_min <- as.numeric(orig_data$temp_min)
orig_data$temp_max <- as.numeric(orig_data$temp_max)
orig_data$precip <- as.numeric(orig_data$precip)
orig_data$weekend <- as.factor(orig_data$weekend)
orig_data$consum <- as.numeric(as.character(orig_data$consum))
```

``` {r orig_summary, echo=FALSE}
summary(orig_data)
```

``` {r orig_add_month, echo=FALSE}
# Add month column
orig_data <- orig_data %>%
  mutate(mo = month(date))
```

``` {r orig_distr, echo=FALSE}
orig_p1 <- ggplot(orig_data, aes(x = temp_median)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Median Temperature",
       x = "Temperature (C)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(temp_median)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(temp_median)),col='green',linewidth=0.5)

orig_p2 <- ggplot(orig_data, aes(x = temp_min)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Min. Temperature",
       x = "Temperature (C)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(temp_min)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(temp_min)),col='green',linewidth=0.5)

orig_p3 <- ggplot(orig_data, aes(x = temp_max)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Max. Temperature",
       x = "Temperature (C)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(temp_max)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(temp_max)),col='green',linewidth=0.5)

orig_p4 <- ggplot(orig_data, aes(x = precip)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Precipitation",
       x = "Precipitation (mm)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(precip)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(precip)),col='green',linewidth=0.5)

orig_p5 <- ggplot(orig_data, aes(x = weekend)) +
  geom_bar() +
  labs(title = "Count of Weekdays vs Weekends",
       x = "Weekend",
       y = "Count")

orig_p6 <- ggplot(orig_data, aes(x = consum)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Beer Consumption",
       x = "Consumption (litres)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(consum)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(consum)),col='green',linewidth=0.5)

orig_patchwork <- orig_p1 + orig_p2 + orig_p3 + orig_p4 + orig_p5 + orig_p6
orig_patchwork + plot_annotation(
  title = 'Original Dataset Overview',
  subtitle = 'Red Line - Mean, Green Line - Median'
)
```

``` {r orig_time, echo=FALSE}
orig_time_p1 <- ggplot(orig_data, aes(x = mo, y = temp_median, group = mo)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(title = "Median Temperature by Month",
       x = "Month",
       y = "Median Temperature (C)")

orig_time_p2 <- ggplot(orig_data, aes(x = mo, y = temp_min, group = mo)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(title = "Minimum Temperature by Month",
       x = "Month",
       y = "Minimum Temperature (C)")

orig_time_p3 <- ggplot(orig_data, aes(x = mo, y = temp_max, group = mo)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(title = "Maximum Temperature by Month",
       x = "Month",
       y = "Maximum Temperature (C)")

orig_time_p4 <- ggplot(orig_data, aes(x = mo, y = precip, group = mo)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(title = "Precipitation by Month",
       x = "Month",
       y = "Precipitation (mm)")

orig_time_p5 <- ggplot(orig_data, aes(x = mo, y = consum, group = mo)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(title = "Beer Consumption by Month",
       x = "Month",
       y = "Beer Consumption (litres)")

orig_patchwork <- orig_time_p1 + orig_time_p2 + orig_time_p3 + 
  orig_time_p4 + orig_time_p5
orig_patchwork + plot_annotation(
  title = 'Continuous Variables by Month'
)
```

## Method

The simulated data was created by studying the distribution of each variable by month. This approach was used as it would not make sense for the range of temperatures to be random throughout the year as would be the case if we just used the entire dataset's distribution. 

To simulate the temperature, precipitation and beer consumption data a normal distribution was created for each day of the year using the mean and standard deviation values from the source data for the corresponding month.

The simulated precipitation data ended up producing many values less than zero which would not make sense. To rectify this the negative values were set to zero. This worked well and resulted in a distribution close to that of the original dataset.

The continuous variables were all appended together in month order. A sequence of dates throughout the year were then generated and added to the dataset. Finally, the weekend column was created by performing a simple check against the date of whether it fell on a weekend and setting the column to values of either "weekend" or "weekday". This was a slight change to the original dataset which used a binary value for this column. A month number column was also added.

``` {r sim_date, echo=FALSE}
# Start date
start_date <- ymd("2015/01/01")
  
# End date
end_date <- ymd("2015/12/31")
  
# Create range of dates
dates <- seq(start_date, end_date,"days")
```

``` {r sim_temp_median, echo=FALSE}
# Get mean and standard deviation for each month
temp_med_stat <- orig_data %>%
  group_by(mo) %>%
  summarise(mean = mean(temp_median), sd = sd(temp_median))

# Create distributions
temp_med_jan <- rnorm(31, mean=temp_med_stat$mean[[1]], sd=temp_med_stat$sd[[1]])
temp_med_feb <- rnorm(28, mean=temp_med_stat$mean[[2]], sd=temp_med_stat$sd[[2]])
temp_med_mar <- rnorm(31, mean=temp_med_stat$mean[[3]], sd=temp_med_stat$sd[[3]])
temp_med_apr <- rnorm(30, mean=temp_med_stat$mean[[4]], sd=temp_med_stat$sd[[4]])
temp_med_may <- rnorm(31, mean=temp_med_stat$mean[[5]], sd=temp_med_stat$sd[[5]])
temp_med_jun <- rnorm(30, mean=temp_med_stat$mean[[6]], sd=temp_med_stat$sd[[6]])
temp_med_jul <- rnorm(31, mean=temp_med_stat$mean[[7]], sd=temp_med_stat$sd[[7]])
temp_med_aug <- rnorm(31, mean=temp_med_stat$mean[[8]], sd=temp_med_stat$sd[[8]])
temp_med_sep <- rnorm(30, mean=temp_med_stat$mean[[9]], sd=temp_med_stat$sd[[9]])
temp_med_oct <- rnorm(31, mean=temp_med_stat$mean[[10]], sd=temp_med_stat$sd[[10]])
temp_med_nov <- rnorm(30, mean=temp_med_stat$mean[[11]], sd=temp_med_stat$sd[[11]])
temp_med_dec <- rnorm(31, mean=temp_med_stat$mean[[12]], sd=temp_med_stat$sd[[12]])

# Combine into a single vector
temp_med <- c(temp_med_jan, temp_med_feb, temp_med_mar, temp_med_apr,
                   temp_med_may, temp_med_jun, temp_med_jul, temp_med_aug,
                   temp_med_sep, temp_med_oct, temp_med_nov, temp_med_dec)
```

``` {r sim_temp_min, echo=FALSE}
# Get mean and standard deviation for each month
temp_min_stat <- orig_data %>%
  group_by(mo) %>%
  summarise(mean = mean(temp_min), sd = sd(temp_min))

# Create distributions
temp_min_jan <- rnorm(31, mean=temp_min_stat$mean[[1]], sd=temp_min_stat$sd[[1]])
temp_min_feb <- rnorm(28, mean=temp_min_stat$mean[[2]], sd=temp_min_stat$sd[[2]])
temp_min_mar <- rnorm(31, mean=temp_min_stat$mean[[3]], sd=temp_min_stat$sd[[3]])
temp_min_apr <- rnorm(30, mean=temp_min_stat$mean[[4]], sd=temp_min_stat$sd[[4]])
temp_min_may <- rnorm(31, mean=temp_min_stat$mean[[5]], sd=temp_min_stat$sd[[5]])
temp_min_jun <- rnorm(30, mean=temp_min_stat$mean[[6]], sd=temp_min_stat$sd[[6]])
temp_min_jul <- rnorm(31, mean=temp_min_stat$mean[[7]], sd=temp_min_stat$sd[[7]])
temp_min_aug <- rnorm(31, mean=temp_min_stat$mean[[8]], sd=temp_min_stat$sd[[8]])
temp_min_sep <- rnorm(30, mean=temp_min_stat$mean[[9]], sd=temp_min_stat$sd[[9]])
temp_min_oct <- rnorm(31, mean=temp_min_stat$mean[[10]], sd=temp_min_stat$sd[[10]])
temp_min_nov <- rnorm(30, mean=temp_min_stat$mean[[11]], sd=temp_min_stat$sd[[11]])
temp_min_dec <- rnorm(31, mean=temp_min_stat$mean[[12]], sd=temp_min_stat$sd[[12]])

# Combine into a single vector
temp_min_df <- c(temp_min_jan, temp_min_feb, temp_min_mar, temp_min_apr,
                   temp_min_may, temp_min_jun, temp_min_jul, temp_min_aug,
                   temp_min_sep, temp_min_oct, temp_min_nov, temp_min_dec)
```

``` {r sim_temp_max, echo=FALSE}
# Get mean and standard deviation for each month
temp_max_stat <- orig_data %>%
  group_by(mo) %>%
  summarise(mean = mean(temp_max), sd = sd(temp_max))

# Create distributions
temp_max_jan <- rnorm(31, mean=temp_max_stat$mean[[1]], sd=temp_max_stat$sd[[1]])
temp_max_feb <- rnorm(28, mean=temp_max_stat$mean[[2]], sd=temp_max_stat$sd[[2]])
temp_max_mar <- rnorm(31, mean=temp_max_stat$mean[[3]], sd=temp_max_stat$sd[[3]])
temp_max_apr <- rnorm(30, mean=temp_max_stat$mean[[4]], sd=temp_max_stat$sd[[4]])
temp_max_may <- rnorm(31, mean=temp_max_stat$mean[[5]], sd=temp_max_stat$sd[[5]])
temp_max_jun <- rnorm(30, mean=temp_max_stat$mean[[6]], sd=temp_max_stat$sd[[6]])
temp_max_jul <- rnorm(31, mean=temp_max_stat$mean[[7]], sd=temp_max_stat$sd[[7]])
temp_max_aug <- rnorm(31, mean=temp_max_stat$mean[[8]], sd=temp_max_stat$sd[[8]])
temp_max_sep <- rnorm(30, mean=temp_max_stat$mean[[9]], sd=temp_max_stat$sd[[9]])
temp_max_oct <- rnorm(31, mean=temp_max_stat$mean[[10]], sd=temp_max_stat$sd[[10]])
temp_max_nov <- rnorm(30, mean=temp_max_stat$mean[[11]], sd=temp_max_stat$sd[[11]])
temp_max_dec <- rnorm(31, mean=temp_max_stat$mean[[12]], sd=temp_max_stat$sd[[12]])

# Combine into a single vector
temp_max_df <- c(temp_max_jan, temp_max_feb, temp_max_mar, temp_max_apr,
                   temp_max_may, temp_max_jun, temp_max_jul, temp_max_aug,
                   temp_max_sep, temp_max_oct, temp_max_nov, temp_max_dec)
```

``` {r sim_precip, echo=FALSE}
# Get mean and standard deviation for each month
precip_stat <- orig_data %>%
  group_by(mo) %>%
  summarise(mean = mean(precip), sd = sd(precip))

# Create distributions
precip_jan <- rnorm(31, mean=precip_stat$mean[[1]], sd=precip_stat$sd[[1]])
precip_feb <- rnorm(28, mean=precip_stat$mean[[2]], sd=precip_stat$sd[[2]])
precip_mar <- rnorm(31, mean=precip_stat$mean[[3]], sd=precip_stat$sd[[3]])
precip_apr <- rnorm(30, mean=precip_stat$mean[[4]], sd=precip_stat$sd[[4]])
precip_may <- rnorm(31, mean=precip_stat$mean[[5]], sd=precip_stat$sd[[5]])
precip_jun <- rnorm(30, mean=precip_stat$mean[[6]], sd=precip_stat$sd[[6]])
precip_jul <- rnorm(31, mean=precip_stat$mean[[7]], sd=precip_stat$sd[[7]])
precip_aug <- rnorm(31, mean=precip_stat$mean[[8]], sd=precip_stat$sd[[8]])
precip_sep <- rnorm(30, mean=precip_stat$mean[[9]], sd=precip_stat$sd[[9]])
precip_oct <- rnorm(31, mean=precip_stat$mean[[10]], sd=precip_stat$sd[[10]])
precip_nov <- rnorm(30, mean=precip_stat$mean[[11]], sd=precip_stat$sd[[11]])
precip_dec <- rnorm(31, mean=precip_stat$mean[[12]], sd=precip_stat$sd[[12]])

precip_df <- c(precip_jan, precip_feb, precip_mar, precip_apr,
                   precip_may, precip_jun, precip_jul, precip_aug,
                   precip_sep, precip_oct, precip_nov, precip_dec)

# Combine into a single vector
precip_df <- replace(precip_df, precip_df < 0, 0)
```

``` {r sim_consum, echo=FALSE}
# Get mean and standard deviation for each month
consum_stat <- orig_data %>%
  group_by(mo) %>%
  summarise(mean = mean(consum), sd = sd(consum))

# Create distributions
consum_jan <- rnorm(31, mean=consum_stat$mean[[1]], sd=consum_stat$sd[[1]])
consum_feb <- rnorm(28, mean=consum_stat$mean[[2]], sd=consum_stat$sd[[2]])
consum_mar <- rnorm(31, mean=consum_stat$mean[[3]], sd=consum_stat$sd[[3]])
consum_apr <- rnorm(30, mean=consum_stat$mean[[4]], sd=consum_stat$sd[[4]])
consum_may <- rnorm(31, mean=consum_stat$mean[[5]], sd=consum_stat$sd[[5]])
consum_jun <- rnorm(30, mean=consum_stat$mean[[6]], sd=consum_stat$sd[[6]])
consum_jul <- rnorm(31, mean=consum_stat$mean[[7]], sd=consum_stat$sd[[7]])
consum_aug <- rnorm(31, mean=consum_stat$mean[[8]], sd=consum_stat$sd[[8]])
consum_sep <- rnorm(30, mean=consum_stat$mean[[9]], sd=consum_stat$sd[[9]])
consum_oct <- rnorm(31, mean=consum_stat$mean[[10]], sd=consum_stat$sd[[10]])
consum_nov <- rnorm(30, mean=consum_stat$mean[[11]], sd=consum_stat$sd[[11]])
consum_dec <- rnorm(31, mean=consum_stat$mean[[12]], sd=consum_stat$sd[[12]])

# Combine into a single vector
consum_df <- c(consum_jan, consum_feb, consum_mar, consum_apr,
                   consum_may, consum_jun, consum_jul, consum_aug,
                   consum_sep, consum_oct, consum_nov, consum_dec)
```

``` {r sim_data, echo=FALSE}
# Create the data
sim_data <- tibble(
  date = dates,
  temp_median = temp_med,
  temp_min = temp_min_df,
  temp_max = temp_max_df,
  precip = precip_df,
  consum = consum_df
) %>%
  mutate(across(where(is.numeric), round, digits = 2))

# Add month column
sim_data <- sim_data %>%
  mutate(mo = month(date))

# Add the weekend column
sim_data <- sim_data %>%
  mutate(weekend = ifelse(wday(date, week_start = 1) %in% c(6, 7), "Weekend", "Weekday"))
```

We then plotted the simulated dataset in the same format as done for the original in order to form a comparison. Visually they looked quite similar and would work for our purposes in answering the hypothesis.

``` {r sim_distr, echo=FALSE}
sim_dist_p1 <- ggplot(sim_data, aes(x = temp_median)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Median Temperature",
       x = "Temperature (C)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(temp_median)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(temp_median)),col='green',linewidth=0.5)

sim_dist_p2 <- ggplot(sim_data, aes(x = temp_min)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Min. Temperature",
       x = "Temperature (C)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(temp_min)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(temp_min)),col='green',linewidth=0.5)

sim_dist_p3 <- ggplot(sim_data, aes(x = temp_max)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Max. Temperature",
       x = "Temperature (C)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(temp_max)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(temp_max)),col='green',linewidth=0.5)

sim_dist_p4 <- ggplot(sim_data, aes(x = precip)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Precipitation",
       x = "Precipitation (mm)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(precip)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(precip)),col='green',linewidth=0.5)

sim_dist_p5 <- ggplot(orig_data, aes(x = weekend)) +
  geom_bar() +
  labs(title = "Count of Weekdays vs Weekends",
       x = "Weekend",
       y = "Count")

sim_dist_p6 <- ggplot(sim_data, aes(x = consum)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of Beer Consumption",
       x = "Consumption (litres)",
       y = "Count") +
  geom_vline(aes(xintercept = mean(consum)),col='red',linewidth=0.5) +
  geom_vline(aes(xintercept = median(consum)),col='green',linewidth=0.5)

sim_patchwork_1 <- sim_dist_p1 + sim_dist_p2 + sim_dist_p3 + sim_dist_p4 +
  sim_dist_p5 + sim_dist_p6
sim_patchwork_1 + plot_annotation(
  title = 'Simulated Dataset Overview',
  subtitle = 'Red Line - Mean, Green Line - Median'
)
```

Our hypothesis relies upon assessing the relationship between two clearly defined continuous quantitative variables therefore the statistical test chosen was linear regression.

### Correlation

We first created a correlation matrix using Pearson's method to understand the strength of correlations between each variable as a multiple linear relationship could have been more appropriate. None of the variables had a strong correlation with beer consumption. The temperature variables were all around the 0.17-0.20 range with precipitation even weaker at 0.13. As we would expect the temperature variables all showed signs of multicollinearity with each other.

``` {r sim_corr_plot, echo=FALSE}
# Select the columns
cor_matrix <- sim_data %>%
  select(temp_median:consum)

# Create the correlation matrix
cor_matrix <- round(cor(cor_matrix, use = "complete.obs"), 2)

# Show matrix
cor_matrix

# Plot matrix
corrplot(cor_matrix, method="circle", type = "upper")
```

### Detail Plots of Each Variable

We plotted scatter plots for each of the quantitative variables to get a visual understanding of any patterns emerging.

##### Correlation Value
``` {r temp_med_corr, echo=FALSE}

cor(sim_data$temp_median, sim_data$consum)

p_corr_temp_med <- ggplot(sim_data, aes(x=temp_median, y=consum)) + 
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", linewidth = 1, formula = y ~ x) +
  geom_point() +
  labs(x="Median Temperature (C)", y = "Beer Consumption (litres)")

p_corr_temp_med
```

Median temperature showed a large variance and also a clustering around the middle of the plot. However, there is a very slight positive linear trend.

### Minimum Temperature

##### Correlation Value
``` {r temp_min_corr, echo=FALSE}

cor(sim_data$temp_min, sim_data$consum)

p_corr_temp_min <- ggplot(sim_data, aes(x=temp_min, y=consum)) + 
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", linewidth = 1, formula = y ~ x) +
  geom_point() +
  labs(x="Min. Temperature (C)", y = "Beer Consumption (litres)")

p_corr_temp_min
```

Minimum temperature had a similar pattern as median temperature but with less clustering and even more variation.

### Maximum Temperature

##### Correlation Value
``` {r temp_max_corr, echo=FALSE}

cor(sim_data$temp_max, sim_data$consum)

p_corr_temp_max <- ggplot(sim_data, aes(x=temp_max, y=consum)) + 
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", linewidth = 1, formula = y ~ x) +
  geom_point() +
  labs(x="Max. Temperature (C)", y = "Beer Consumption (litres)")

p_corr_temp_max
```

Maximum temperature was very similar to the median temperature plot with the same high levels of variation and a clustering around the middle.

### Precipitation

##### Correlation Value
``` {r precip_corr, echo=FALSE}

cor(sim_data$precip, sim_data$consum)

p_corr_precip <- ggplot(sim_data, aes(x=precip, y=consum)) + 
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", linewidth = 1, formula = y ~ x) +
  geom_point() +
  labs(x="Precipitation (mm)", y = "Beer Consumption (litres)")

p_corr_precip
```

Precipitation was a very different story to the temperature variables and had a cluster around the zero mark which would make sense as it would not rain every day. The regression line added showed a slight positive trend but the variation would appear to increase as the precipitation increased.

### Month

##### Correlation Value
``` {r mo_corr, echo=FALSE}

cor(sim_data$mo, sim_data$consum)

ggplot(sim_data, aes(x = mo, y = consum, group = mo)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(x="Month", y = "Beer Consumption (litres)")
```

``` {r consum_time, echo=FALSE}
ggplot(sim_data, aes(x = date, y = consum)) + 
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", linewidth = 1, formula = y ~ x) +
  geom_point() +
  geom_vline(xintercept = as.numeric(as.Date(c("2015-04-01","2015-07-01", "2015-10-01"))),
               linetype = "dashed", linewidth = 0.5, col = "gray")+
    annotate("text", x = as.Date("2015-02-01"), y = 8, label = "Summer", size = 5, col = "gray")+
    annotate("text", x = as.Date("2015-05-01"), y = 8, label = "Autumn", size = 5, col = "gray")+
    annotate("text", x = as.Date("2015-08-01"), y = 8, label = "Winter", size = 5, col = "gray")+
    annotate("text", x = as.Date("2015-11-01"), y = 8, label = "Spring", size = 5, col = "gray") +
  labs(x="Date", y = "Beer Consumption (litres)")
```

``` {r mo_consum, echo=FALSE}

ggplot(sim_data, aes(x = mo, y = consum, group = mo)) +
  geom_col() +
  scale_x_continuous(breaks = seq(1,12,1)) +
  labs(x="Month", y = "Beer Consumption (litres)")
```

Looking at beer consumption by month showed a clear pattern of higher consumption generally in the spring and summer months which is not surprising. 

### Weekdays vs Weekends

``` {r weekend_corr, echo=FALSE}
ggplot(sim_data, aes(x = weekend, y = consum, group = weekend)) +
  geom_boxplot() +
  stat_summary(fun = "mean", aes(ymax = after_stat(y), ymin = after_stat(y)), 
               color = "red", shape = 4) +
  labs(y = "Beer Consumption (litres)")
```

Beer consumption appeared to be at similar levels during the week and at the weekend.

### Simple Linear Regression
Simple linear regression of beer consumption was then performed using the median temperature as the predictor variable. The relationship between variables was then assessed by reviewing the P-Value and R-Squared value. We then looked at the residuals plot to check for patterns and the Q-Q plot to check whether the data was normally distributed.

``` {r lm_temp_med, echo=FALSE}

temp_med_lm <- lm(formula = consum ~ temp_median, data = sim_data)

tidy(temp_med_lm)
# Coefficients
# Intercept: 20,5
# Slope: 0.239

glance(temp_med_lm)
# R-Squared - 0.0316

autoplot(temp_med_lm)[1:2]
# No clear pattern within the residuals and the qq plot looks normally distributed

summary(temp_med_lm)
# P-Value: 0.0006445
```

The p-value is low which suggests a significant relationship if our confidence level is to be set at 95%. The r-squared value is also low meaning that the relationship does not explain the variance within the data. Analysis of the residuals plot showed no clear pattern which signifies that errors are normally distributed. The Q-Q plot suggests a normal distribution also.

### Multiple Linear Regression

Due to the large variance in the simple linear regression model I thought it might be interesting to add other variables and try a multiple regression model to see if this could improve the accuracy. However, I recognise that due to the multicollinearity of the temperature variables this probably would not be ideal but I thought it best to try another angle.

We decided to use a forward selection approach starting with the median temperature already included. This was done as there are a small number of variables in the data and it would be more efficient and we already had a base model with a single variable as the predictor. As this was a multiple linear regression test any comparisons would use the adjusted r-squared value of each model including when comparing to the simple linear regression model.

``` {r lm, echo=FALSE}

# First layer models
lm_1 <- lm(consum ~ temp_med + temp_min, data = sim_data)
lm_2 <- lm(consum ~ temp_med + temp_max, data = sim_data)
lm_3 <- lm(consum ~ temp_med + precip, data = sim_data)

# Combine models
models <- list(lm_1, lm_2, lm_3)

models_r_sqr <- models %>%
  map_df(function(linear_model) {
    tibble(
      the_model = as.character(linear_model$call)[2],
      adj_r_sqr = pull(glance(linear_model), adj.r.squared)
    )
  })

arrange(models_r_sqr, desc(adj_r_sqr))
```

The baseline adjusted r-squared of the simple linear regression model with just median temperature was 0.02894. Adding in minimum temperature increased this to 0.0427 becoming the new baseline for the next round of variable addition.

``` {r lm_1, echo=FALSE}

# Second layer models
lm_1_1 <- lm(consum ~ temp_med + temp_min + temp_max, data = sim_data)
lm_1_2 <- lm(consum ~ temp_med + temp_min + precip, data = sim_data)

# Combine models
models_1 <- list(lm_1_1, lm_1_2)

models_1_r_sqr <- models_1 %>%
  map_df(function(linear_model) {
    tibble(
      the_model = as.character(linear_model$call)[2],
      adj_r_sqr = pull(glance(linear_model), adj.r.squared)
    )
  })

arrange(models_1_r_sqr, desc(adj_r_sqr))
```

The best model after this round had an adjusted r-squared of 0.0459 and used median, minimum and maximum temperature. The p-value of this model was lower at 0.0001703 compared to 0.0006445 of the original model using just median temperature. The Q-Q plot showed a normal distribution and the residuals plot displayed no clear pattern.

``` {r lm_1_analysis, echo=FALSE}

# P-Value: 0.0001703
summary(lm_1_1)

autoplot(lm_1_1)[1:2]
```

## Results

Simple linear regression analysis produced a low p-value and low r-squared value which tells us the relationship is significant between median temperature and beer consumption but does not explain the variance we could see in the scatter plot.

A comparison between the adjusted r-squared values of both the simple and multiple linear regression models reveals that statistically the multiple regression model is marginally better.

``` {r models_final, echo=FALSE}

# Comparison of simple and multiple linear regression
models_final <- list(temp_med_lm, lm_1_1)

final_r_sqr <- models_final %>%
  map_df(function(linear_model) {
    tibble(
      the_model = as.character(linear_model$call)[2],
      adj_r_sqr = pull(glance(linear_model), adj.r.squared)
    )
  })

arrange(final_r_sqr, desc(adj_r_sqr))
```

## Conclusion

The simple linear regression analysis gave us a p-value of 0.0006445 which tells us that the relationship between temperature and beer consumption is significant and therefore we should reject the null hypotheses. However, the low R-Squared value of 0.0316 suggests that the simple linear regression model does not explain the variance in our data. This is evident when looking at a scatter plot of the data.

``` {r conclusion_1, echo=FALSE}
p_corr_temp_med
```

Moving to a multiple linear regression model improved the p-value to 0.0001703 and also improved the adjusted r-squared from 0.0289 to 0.0459. The multiple linear regression model used variables that were all multicollinear which can lead to less reliable results (Hayes 2022). This model also increased complexity with the addition of extra variables.

The improvements gained by using multiple linear regression were marginal and do not change my analysis that we could loosely say that temperature does have an effect on beer consumption but the results don't confirm causation.

This analysis focussed on answering the hypothesis defined at the start in understanding if there was any relationship between temperature and beer consumption among students but there are opportunities for further work within this data if the scope was expanded. For example the time of year showed a promising pattern that could be interesting to explore.

## References

Hayes, A (2022) Multicollinearity [online] available at: https://www.investopedia.com/terms/m/multicollinearity.asp [23rd Dec 2022]

Lustosa, A.G. (2018) Beer Consumption - Sao Paulo [online] available at: https://www.kaggle.com/datasets/dongeorge/beer-consumption-sao-paulo [10th Dec 2022]




